{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9ce5ad",
   "metadata": {},
   "source": [
    "# Mockup of use cases and vision for CEDA adoption of GeoCroissant\n",
    "\n",
    "The Centre for Environmental Data Analysis (CEDA), and its partner UK environmental data centres, are working on multiple projects aimed at making their data more _AI-ready_. What we mean by _AI-readiness_ is that the data should be:\n",
    "- easy to find\n",
    "- easy to access\n",
    "- efficient to process/load at scale\n",
    "- integrated with local/remote performant caching\n",
    "- easy to transform and load into Machine Learning workflows\n",
    "- easy for Agentic AI to interact with\n",
    "- self-describing in terms of its characteristics in relation to usage, such as:\n",
    "  - caveats on usage\n",
    "  - consideration of data quality and uncertainty\n",
    "  - clarification of biases in the collection and construction of the data\n",
    "\n",
    "These characteristics are highlighted in the following sections of this Notebook:\n",
    "1. Discover, search and query\n",
    "2. Interrogate the contents of a dataset\n",
    "3. Filter and subset\n",
    "4. Extract, transform and load\n",
    "5. Copying data to a local cache\n",
    "6. Usage warnings and caveats (at _global_ and _variable_ levels)\n",
    "7. Integration with ML packages (PyTorch)\n",
    "8. Agentic access (via MCP)\n",
    "9. Accessing local and/or remote data (file system vs S3/HTTP)\n",
    "10. Handling restricted data with access control\n",
    "11. Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c33e1",
   "metadata": {},
   "source": [
    "### Firstly, we'll make some imports to set up the Notebook\n",
    "\n",
    "**NOTE: this is a synthetic notebook that uses _mock_ packages. It is intended as a useful tool for describing (and proposing) a narrative on how `geocroissant` might work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82dd9a47",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "\n",
      "Croissant version: 1.2.3\n",
      "PyTorch version: 2.1.0\n",
      "Xarray version: 2023.10.1\n",
      "Cartopy version: 0.22.0\n",
      "STAC Client version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries from the external mock module\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current directory to Python path to find mocklib.py\n",
    "current_dir = os.path.dirname(os.path.abspath('.'))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "# Import from our mock module\n",
    "from mocklib import (\n",
    "    croissant, torch, xr, STACIntegration, DataLoader, Dataset,\n",
    "    torch_nn as nn, matplotlib_pyplot as plt, cartopy_crs as ccrs, \n",
    "    cartopy_feature as cfeature, pystac_client as Client, ceda_auth\n",
    ")\n",
    "\n",
    "# Standard libraries (these are real)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\\n\")\n",
    "print(f\"Croissant version: {croissant.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Xarray version: {xr.__version__}\")\n",
    "print(f\"Cartopy version: 0.22.0\")\n",
    "print(f\"STAC Client version: 0.7.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18604a7",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 1. Discover, search and query\n",
    "\n",
    "At the top level, users should have a single Python API from which they can explore _all data_. In this example, we imagine that there is a `GeoCroissant` object imported from `croissant` that you can create an instance of by giving it the URL to a (Geo-)Croissant catalogue.\n",
    "\n",
    "The end-point serves up _geo-aware_ dataset records that can be interrogated.\n",
    "\n",
    "Note that the `GeoCroissant` object can be interrogated in multiple ways:\n",
    "1. Using a built-in operations for space and time:\n",
    "  - `spatial_coverage`\n",
    "  - `temporal_coverage`\n",
    "2. By keywords - based on those tagged in the datasets\n",
    "3. By _facets_:\n",
    "  - Picking up domain-specific vocabularies for different datasets, such as:\n",
    "    - Satellite data: `sensor_id`, `platform`\n",
    "    - Climate simulations: `ensemble_member`, `grid_type`, `frequency`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af83b986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing GeoCroissant client using provider: https://catalogue.ceda.ac.uk/croissant/\n",
      "Found 3 matching datasets:\n",
      "\n",
      "1. CMIP6_Global_Climate_Projections\n",
      "     Description: Multi-model ensemble of global climate projections from CMIP6\n",
      "     Provider: ESGF Data Nodes\n",
      "     Variables: temperature, precipitation, pressure...\n",
      "     Spatial Resolution: 1.25¬∞ x 1.25¬∞\n",
      "     Temporal Resolution: monthly\n",
      "\n",
      "2. ERA5_Reanalysis_Global\n",
      "     Description: ECMWF ERA5 atmospheric reanalysis dataset\n",
      "     Provider: Copernicus Climate Data Store\n",
      "     Variables: temperature, wind, pressure...\n",
      "     Spatial Resolution: 0.25¬∞ x 0.25¬∞\n",
      "     Temporal Resolution: hourly\n",
      "\n",
      "3. MODIS_Land_Surface_Temperature\n",
      "     Description: MODIS satellite-derived land surface temperature\n",
      "     Provider: NASA EARTHDATA\n",
      "     Variables: land_surface_temperature, emissivity...\n",
      "     Spatial Resolution: 1km\n",
      "     Temporal Resolution: daily\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize GeoCroissant with multiple data sources\n",
    "geocat = croissant.GeoCroissant(provider=\"https://catalogue.ceda.ac.uk/croissant/\")  # type: ignore\n",
    "\n",
    "# Search for climate datasets\n",
    "datasets = geocat.search(\n",
    "    spatial_coverage=[-30, -10, 40, 30],  # Example bounding box [min_lon, min_lat, max_lon, max_lat]\n",
    "    temporal_range=(\"2015-01-01\", \"2100-12-31\"),\n",
    "    keywords=[\"climate\", \"temperature\", \"precipitation\"],\n",
    "    # facets={\"model\": [\"UKESM1-0-LL\", \"HadGEM3-GC31-LL\"]},\n",
    ")\n",
    "\n",
    "print(f\"Found {len(datasets)} matching datasets:\\n\")\n",
    "for i, dataset in enumerate(datasets[:5]):  # Show first 5\n",
    "    print(f\"{i+1}. {dataset.name}\")\n",
    "    print(f\"     Description: {dataset.description}\")\n",
    "    print(f\"     Provider: {dataset.provider}\")\n",
    "    print(f\"     Variables: {', '.join(dataset.variables[:3])}...\")\n",
    "    print(f\"     Spatial Resolution: {dataset.spatial_resolution}\")\n",
    "    print(f\"     Temporal Resolution: {dataset.temporal_resolution}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da27603",
   "metadata": {},
   "source": [
    "## 2. Interrogate the contents of a dataset\n",
    "\n",
    "The catalogue and dataset objects expose methods that allow the user to directly interrogate them regarding their contents. \n",
    "\n",
    "Initially, `<dataset>.get_props(\"__available__\")` returns a list of the possible properties (or _facets_) that the dataset exposes. After that call, the user can use `<dataset>.get_props(\"<prop_name>\")` to find out which values can be selected for each property.\n",
    "\n",
    "**NOTE: A warning appears to provide guidance on how the data can/cannot be used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0ba22f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background:#ffe6e6;border:2px solid #d32f2f;padding:16px;border-radius:8px;text-align:center;\">\n",
       "            <span style=\"color:#d32f2f;font-size:1.5em;font-weight:bold;\">‚ö†Ô∏è Important information about the CMIP6 Dataset</span><br>\n",
       "            <span style=\"color:#333;font-size:1.1em;\">The CMIP6 Dataset has the following important factors:\n",
       "\n",
       "    - It is a multi-model ensemble of global climate projections.\n",
       "    - The dataset includes variables such as temperature, precipitation, and wind.\n",
       "    - It is available at a spatial resolution of 1.25¬∞ x 1.25¬∞.\n",
       "    - Different models will have varying temporal coverages and spatial resolutions.\n",
       "    See: more information at <a href=\"https://esgf-node.llnl.gov/projects/cmip6/\">https://esgf-node.llnl.gov/projects/cmip6/</a>\n",
       "        </span>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Interrogating CMIP6 dataset contents...\n",
      "üîß Other Available Properties:\n",
      "Available props: models, experiments, variables, frequencies, realms, institutions, grids, time_ranges\n",
      "Use cmip6_dataset.get_props('property_name') to explore any of these:\n",
      "üìä Available Climate Models (5):\n",
      "  - CESM2: NCAR\n",
      "    Resolution: 0.9x1.25 deg\n",
      "    Experiments: 3\n",
      "\n",
      "  - GFDL-ESM4: NOAA-GFDL\n",
      "    Resolution: 0.5 deg\n",
      "    Experiments: 4\n",
      "\n",
      "  - UKESM1-0-LL: MOHC\n",
      "    Resolution: 1.25x1.875 deg\n",
      "    Experiments: 3\n",
      "\n",
      "  - IPSL-CM6A-LR: IPSL\n",
      "    Resolution: 1.27x2.5 deg\n",
      "    Experiments: 4\n",
      "\n",
      "  - MPI-ESM1-2-HR: MPI-M\n",
      "    Resolution: 0.94x0.94 deg\n",
      "    Experiments: 3\n",
      "\n",
      "üß™ Available Experiments (5):\n",
      "  - ssp126: Low emissions scenario\n",
      "    Activity: ScenarioMIP\n",
      "    Models: 12\n",
      "\n",
      "  - ssp245: Medium emissions scenario\n",
      "    Activity: ScenarioMIP\n",
      "    Models: 15\n",
      "\n",
      "  - ssp370: Medium-high emissions scenario\n",
      "    Activity: ScenarioMIP\n",
      "    Models: 8\n",
      "\n",
      "  - ssp585: High emissions scenario\n",
      "    Activity: ScenarioMIP\n",
      "    Models: 18\n",
      "\n",
      "  - historical: Historical simulation\n",
      "    Activity: CMIP\n",
      "    Models: 25\n",
      "\n",
      "üå°Ô∏è Available Variables (10):\n",
      "  - tas: Near-Surface Air Temperature\n",
      "    Units: K\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - pr: Precipitation\n",
      "    Units: kg m-2 s-1\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - psl: Sea Level Pressure\n",
      "    Units: Pa\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - ua: Eastward Near-Surface Wind\n",
      "    Units: m s-1\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - va: Northward Near-Surface Wind\n",
      "    Units: m s-1\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - huss: Near-Surface Specific Humidity\n",
      "    Units: 1\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - zg: Geopotential Height\n",
      "    Units: m\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'plev', 'lat', 'lon']\n",
      "\n",
      "  - evspsbl: Evaporation\n",
      "    Units: kg m-2 s-1\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - tasmax: Daily Maximum Near-Surface Air Temperature\n",
      "    Units: K\n",
      "    Frequency: day\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - tasmin: Daily Minimum Near-Surface Air Temperature\n",
      "    Units: K\n",
      "    Frequency: day\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "üîß Other Available Properties:\n",
      "Available props: models, experiments, variables, frequencies, realms, institutions, grids, time_ranges\n",
      "Use cmip6_dataset.get_props('property_name') to explore any of these:\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset (e.g., CMIP6)\n",
    "cmip6_dataset = geocat.load_dataset(\"CMIP6_Global_Climate_Projections\")\n",
    "\n",
    "# Use the generic interrogation API to explore the dataset\n",
    "print(\"üîç Interrogating CMIP6 dataset contents...\")\n",
    "\n",
    "# List available properties\n",
    "available_props = cmip6_dataset.get_props(\"__available__\")\n",
    "print(f\"üîß Other Available Properties:\")\n",
    "print(f\"Available props: {', '.join(available_props)}\")\n",
    "print(f\"Use cmip6_dataset.get_props('property_name') to explore any of these:\")\n",
    "\n",
    "# Get available climate models using generic props interface\n",
    "models = cmip6_dataset.get_props(\"models\")\n",
    "print(f\"üìä Available Climate Models ({len(models)}):\")\n",
    "for model in models[:8]:\n",
    "    print(f\"  - {model.name}: {model.institution}\")\n",
    "    print(f\"    Resolution: {model.nominal_resolution}\")\n",
    "    print(f\"    Experiments: {len(model.experiments)}\")\n",
    "    print()\n",
    "\n",
    "# Get available experiments\n",
    "experiments = cmip6_dataset.get_props(\"experiments\")\n",
    "print(f\"üß™ Available Experiments ({len(experiments)}):\")\n",
    "for exp in experiments[:5]:\n",
    "    print(f\"  - {exp.experiment_id}: {exp.description}\")\n",
    "    print(f\"    Activity: {exp.activity_id}\")\n",
    "    print(f\"    Models: {len(exp.participating_models)}\")\n",
    "    print()\n",
    "\n",
    "# Get available variables\n",
    "variables = cmip6_dataset.get_props(\"variables\")\n",
    "print(f\"üå°Ô∏è Available Variables ({len(variables)}):\")\n",
    "for var in variables[:10]:\n",
    "    print(f\"  - {var.variable_id}: {var.long_name}\")\n",
    "    print(f\"    Units: {var.units}\")\n",
    "    print(f\"    Frequency: {var.frequency}\")\n",
    "    print(f\"    Dimensions: {var.dimensions}\")\n",
    "    print()\n",
    "\n",
    "# Show other available properties that can be interrogated\n",
    "available_props = cmip6_dataset.get_props(\"__available__\")\n",
    "print(f\"üîß Other Available Properties:\")\n",
    "print(f\"Available props: {', '.join(available_props)}\")\n",
    "print(f\"Use cmip6_dataset.get_props('property_name') to explore any of these:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3cbc8",
   "metadata": {},
   "source": [
    "## 3. Filter and subset\n",
    "\n",
    "Before any data is actually loaded, the contents of the required dataset can be filtered. This all uses _lazy loading_ which means that the software stores a graph of the required operations which will only be executed when the data arrays themselves are needed (e.g. for model training, analysis or visualisation).\n",
    "\n",
    "Again, this allows the specification of _generic_ properties, such as _space_ and _time_, along with _dataset-specific_ facets such as `model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "245ed933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CMIP6 dataset loaded successfully!\n",
      "Dataset ID: cmip6_global_climate\n",
      "Title: CMIP6 Global Climate Projections\n",
      "Description: Comprehensive climate model data from CMIP6 including temperature, precipitation, and atmospheric variables\n",
      "License: CC-BY-4.0\n",
      "Extent: {'bbox': [-20, 10, 30, 50]}\n",
      "Time Range: {'interval': [['2020-01-01', '2050-12-31']]}\n",
      "\n",
      "üìÅ STAC Catalog Structure:\n",
      "Collections: 3\n",
      "  - temperature: Surface Temperature\n",
      "    Items: 120\n",
      "    Variables: tas, tasmax, tasmin, pr, huss\n",
      "\n",
      "  - precipitation: Precipitation\n",
      "    Items: 120\n",
      "    Variables: pr, prc, prsn, prw, evspsbl\n",
      "\n",
      "  - atmospheric: Atmospheric Variables\n",
      "    Items: 120\n",
      "    Variables: psl, ua, va, zg, hus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CMIP6 dataset with filter options\n",
    "cmip6_dataset = geocat.load_dataset(\n",
    "    \"CMIP6_Global_Climate_Projections\",\n",
    "    spatial_subset=[-20, 10, 30, 50],  # [min_lon, min_lat, max_lon, max_lat]\n",
    "    temporal_subset=(\"2020-01-01\", \"2050-12-31\"),\n",
    "    variables=[\"tas\", \"pr\", \"psl\"],  # Surface air temperature, precipitation and pressure\n",
    "    facets={\"model\": [\"UKESM1-0-LL\", \"HadGEM3-GC31-LL\"]},\n",
    "    suppress_warnings=True,\n",
    ")\n",
    "\n",
    "# The dataset is loaded with STAC integration\n",
    "print(\"‚úÖ CMIP6 dataset loaded successfully!\")\n",
    "print(f\"Dataset ID: {cmip6_dataset.id}\")\n",
    "print(f\"Title: {cmip6_dataset.title}\")\n",
    "print(f\"Description: {cmip6_dataset.description}\")\n",
    "print(f\"License: {cmip6_dataset.license}\")\n",
    "print(f\"Extent: {cmip6_dataset.spatial_extent}\")\n",
    "print(f\"Time Range: {cmip6_dataset.temporal_extent}\")\n",
    "\n",
    "# Show STAC catalog structure\n",
    "print(f\"\\nüìÅ STAC Catalog Structure:\")\n",
    "print(f\"Collections: {len(cmip6_dataset.collections)}\")\n",
    "for collection in cmip6_dataset.collections[:3]:\n",
    "    print(f\"  - {collection.id}: {collection.title}\")\n",
    "    print(f\"    Items: {len(collection.items)}\")\n",
    "    print(f\"    Variables: {', '.join(collection.summaries.get('variables', [])[:5])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729dd429",
   "metadata": {},
   "source": [
    "Or, alternatively, **apply filters after loading a dataset**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b8897ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Applying selection criteria:\n",
      "  model: CESM2\n",
      "  experiment: ssp585\n",
      "  variable: tas\n",
      "  frequency: monthly\n",
      "  spatial_bounds: {'lat': [30, 70], 'lon': [-130, -60]}\n",
      "  temporal_bounds: {'start': '2020-01-01', 'end': '2050-12-31'}\n",
      "\n",
      "üîÑ Filtering dataset...\n",
      "‚úÖ Filtered dataset created!\n",
      "Original size: 1250.0 GB\n",
      "Filtered size: 85.2 GB\n",
      "Reduction: 93.2%\n",
      "\n",
      "üìã Filtered Dataset Structure:\n",
      "Variables: ['tas']\n",
      "Spatial shape: (40, 70)\n",
      "Temporal shape: (372,)\n",
      "Total timesteps: 372\n",
      "Data format: xarray\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset\n",
    "cmip6_dataset = geocat.load_dataset(\"CMIP6_Global_Climate_Projections\", suppress_warnings=True)\n",
    "\n",
    "# Define filtering selection criteria\n",
    "selection_criteria = {\n",
    "    'model': 'CESM2',  # Community Earth System Model\n",
    "    'experiment': 'ssp585',  # High emissions scenario\n",
    "    'variable': 'tas',  # Near-surface air temperature\n",
    "    'frequency': 'monthly',\n",
    "    'spatial_bounds': {\n",
    "        'lat': [30, 70],  # Northern hemisphere focus\n",
    "        'lon': [-130, -60]  # North America\n",
    "    },\n",
    "    'temporal_bounds': {\n",
    "        'start': '2020-01-01',\n",
    "        'end': '2050-12-31'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üéØ Applying selection criteria:\")\n",
    "for key, value in selection_criteria.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Apply the filters using GeoCroissant's filtering API\n",
    "print(\"\\nüîÑ Filtering dataset...\")\n",
    "filtered_dataset = cmip6_dataset.filter(**selection_criteria)\n",
    "\n",
    "# Display summary of the filtered dataset\n",
    "print(f\"‚úÖ Filtered dataset created!\")\n",
    "print(f\"Original size: {cmip6_dataset.estimated_size_gb:.1f} GB\")\n",
    "print(f\"Filtered size: {filtered_dataset.estimated_size_gb:.1f} GB\")\n",
    "print(f\"Reduction: {(1 - filtered_dataset.estimated_size_gb/cmip6_dataset.estimated_size_gb)*100:.1f}%\")\n",
    "\n",
    "# Show the structure of the filtered dataset\n",
    "print(f\"\\nüìã Filtered Dataset Structure:\")\n",
    "print(f\"Variables: {filtered_dataset.variables}\")\n",
    "print(f\"Spatial shape: {filtered_dataset.spatial_shape}\")\n",
    "print(f\"Temporal shape: {filtered_dataset.temporal_shape}\")\n",
    "print(f\"Total timesteps: {filtered_dataset.n_timesteps}\")\n",
    "print(f\"Data format: {filtered_dataset.data_format}\")  # xarray or tensor ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118e425",
   "metadata": {},
   "source": [
    "## 4. Extract, transform and load\n",
    "\n",
    "For use in Machine Learning workflows, the data will often need to be transformed in structure. \n",
    "\n",
    "Transformers can be applied to the `load_dataset(...)` operation, or applied afterwards. In this example, the data is regridded to a 1 degree grid and converted from 64-bit floats (_double_) to 32-bit floats.\n",
    "\n",
    "Additionally, `masked` values are replaced with the mean statistics from each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0ebde60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background:#ffe6e6;border:2px solid #d32f2f;padding:16px;border-radius:8px;text-align:center;\">\n",
       "            <span style=\"color:#d32f2f;font-size:1.5em;font-weight:bold;\">‚ö†Ô∏è Important information about the CMIP6 Dataset</span><br>\n",
       "            <span style=\"color:#333;font-size:1.1em;\">The CMIP6 Dataset has the following important factors:\n",
       "\n",
       "    - It is a multi-model ensemble of global climate projections.\n",
       "    - The dataset includes variables such as temperature, precipitation, and wind.\n",
       "    - It is available at a spatial resolution of 1.25¬∞ x 1.25¬∞.\n",
       "    - Different models will have varying temporal coverages and spatial resolutions.\n",
       "    See: more information at <a href=\"https://esgf-node.llnl.gov/projects/cmip6/\">https://esgf-node.llnl.gov/projects/cmip6/</a>\n",
       "        </span>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CMIP6 dataset prepared to load with transformations applied!\n",
      "Transformations: \n",
      "Transformer type: RegridTransformer, Specification: {'target_grid': '1deg'}\n",
      "Transformer type: TypeCoercionTransformer, Specification: {'dtype': 'float32'}\n",
      "Transformer type: MissingValueImputer, Specification: {'strategy': 'mean'}\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset and apply transformations during the conversion\n",
    "from mocklib import croissant\n",
    "\n",
    "\n",
    "cmip6_dataset = geocat.load_dataset(\n",
    "    \"CMIP6_Global_Climate_Projections\",\n",
    "    spatial_subset=[-20, 10, 30, 50],  # [min_lon, min_lat, max_lon, max_lat]\n",
    "    temporal_subset=(\"2020-01-01\", \"2050-12-31\"),\n",
    "    variables=[\"tas\", \"pr\", \"psl\"],  # Surface air temperature, precipitation and pressure\n",
    "    facets={\"model\": [\"UKESM1-0-LL\", \"HadGEM3-GC31-LL\"]},\n",
    "    transformers=[\n",
    "        croissant.transformers.RegridTransformer(target_grid=\"1deg\"),  # Regrid to 1 degree\n",
    "        croissant.transformers.TypeCoercionTransformer(dtype=\"float32\"),  # Convert to 32-bit floats\n",
    "        croissant.transformers.MissingValueImputer(strategy=\"mean\")  # Impute missing values with mean\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ CMIP6 dataset prepared to load with transformations applied!\")\n",
    "print(\"Transformations: \")\n",
    "for transformer in cmip6_dataset.transformers:\n",
    "    print(transformer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f48c18",
   "metadata": {},
   "source": [
    "## 5. Copying data to a local cache\n",
    "\n",
    "Since large geospatial datasets may be used for many epochs/iterations of model training, it is sometimes necesary to cache the data on local disk. This can be done by providing a `cache_directory \n",
    "\n",
    "Explain caching strategies to optimize repeated access:\n",
    "- Local on-disk and in-memory caches\n",
    "- Remote cache/backing store (S3, HTTP cache-control)\n",
    "- Versioned cache keys and eviction policies\n",
    "- Integration with tooling like fsspec and zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87a23698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to download 500000 data files to cache directory: /disks/storage/data_cache\n",
      "  using 16 worker processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 1/20 [00:00<00:05,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 0 to 25,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 2/20 [00:00<00:05,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 25,000 to 50,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñå        | 3/20 [00:00<00:05,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 50,000 to 75,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 4/20 [00:01<00:04,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 75,000 to 100,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 5/20 [00:01<00:04,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 100,000 to 125,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 6/20 [00:01<00:04,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 125,000 to 150,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:02<00:03,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 150,000 to 175,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:02<00:03,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 175,000 to 200,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:02<00:03,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 200,000 to 225,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:03<00:03,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 225,000 to 250,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:03<00:02,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 250,000 to 275,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:03<00:02,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 275,000 to 300,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:03<00:02,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 300,000 to 325,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:04<00:01,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 325,000 to 350,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:04<00:01,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 350,000 to 375,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:04<00:01,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 375,000 to 400,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:05<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 400,000 to 425,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:05<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 425,000 to 450,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:05<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 450,000 to 475,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:06<00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching files: 475,000 to 500,000\n",
      "\n",
      "\n",
      "Caching completed. 240TiB downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set a cache directory for storing downloaded data\n",
    "CACHE_DIR = \"/disks/storage/data_cache\"\n",
    "\n",
    "# Pre-load all data into the local cache\n",
    "cmip6_dataset.preload(cache_dir=CACHE_DIR, n_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25036ec",
   "metadata": {},
   "source": [
    "## 6. Usage warnings and caveats (at _global_ and _variable_ levels)\n",
    "\n",
    "When building APIs like this, it is important that provenance and usage metadata, including caveats and warnings, are provided to users at the:\n",
    "- Global dataset-level (licence, known biases)\n",
    "- Variable-level (known gaps, quality flags, uncertainty)\n",
    "\n",
    "By default, these are extracted from the metadata records and are exposed to users within the environment they are working in. When using a Jupyter Notebook, they are highlighted as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fc9eabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dataset-level warning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"background:#ffe6e6;border:2px solid #d32f2f;padding:16px;border-radius:8px;text-align:center;\">\n",
       "            <span style=\"color:#d32f2f;font-size:1.5em;font-weight:bold;\">‚ö†Ô∏è Important information about the CMIP6 Dataset</span><br>\n",
       "            <span style=\"color:#333;font-size:1.1em;\">The CMIP6 Dataset has the following important factors:\n",
       "\n",
       "    - It is a multi-model ensemble of global climate projections.\n",
       "    - The dataset includes variables such as temperature, precipitation, and wind.\n",
       "    - It is available at a spatial resolution of 1.25¬∞ x 1.25¬∞.\n",
       "    - Different models will have varying temporal coverages and spatial resolutions.\n",
       "    See: more information at <a href=\"https://esgf-node.llnl.gov/projects/cmip6/\">https://esgf-node.llnl.gov/projects/cmip6/</a>\n",
       "        </span>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"A dataset-level warning:\")\n",
    "cmip6_dataset = geocat.load_dataset(\"CMIP6_Global_Climate_Projections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4f5c346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A variable-level warning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"background:#ffe6e6;border:2px solid #d32f2f;padding:16px;border-radius:8px;text-align:center;\">\n",
       "            <span style=\"color:#d32f2f;font-size:1.5em;font-weight:bold;\">‚ö†Ô∏è Variable-level information about Eastward Near-Surface Wind ('ua')</span><br>\n",
       "            <span style=\"color:#333;font-size:1.1em;\">The Eastward Near-Surface Wind ('ua') variable:\n",
       "            - is provided on a staggered grid when compared to non-wind surface variables.\n",
       "            - has the units m/s\n",
       "            - is calculated as 10-minute mean\n",
       "            </span>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"A variable-level warning:\")\n",
    "ua = cmip6_dataset.variables[\"ua\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc4fc6",
   "metadata": {},
   "source": [
    "The warnings and metadata can also be accessed as properties of the dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47dc91b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings:\n",
      "---------\n",
      "\n",
      "Dataset-level warnings:\n",
      "[{'message': 'The CMIP6 Dataset has the following important factors:\\n'\n",
      "             '\\n'\n",
      "             '    - It is a multi-model ensemble of global climate '\n",
      "             'projections.\\n'\n",
      "             '    - The dataset includes variables such as temperature, '\n",
      "             'precipitation, and wind.\\n'\n",
      "             '    - It is available at a spatial resolution of 1.25¬∞ x 1.25¬∞.\\n'\n",
      "             '    - Different models will have varying temporal coverages and '\n",
      "             'spatial resolutions.\\n'\n",
      "             '    See: more information at <a '\n",
      "             'href=\"https://esgf-node.llnl.gov/projects/cmip6/\">https://esgf-node.llnl.gov/projects/cmip6/</a>\\n'\n",
      "             '        ',\n",
      "  'title': 'Important information about the CMIP6 Dataset'}]\n",
      "\n",
      "Variable-level warnings:\n",
      "[{'message': \"The Eastward Near-Surface Wind ('ua') variable:\\n\"\n",
      "             '            - is provided on a staggered grid when compared to '\n",
      "             'non-wind surface variables.\\n'\n",
      "             '            - has the units m/s\\n'\n",
      "             '            - is calculated as 10-minute mean\\n'\n",
      "             '            ',\n",
      "  'title': 'Variable-level information about Eastward Near-Surface Wind '\n",
      "           \"('ua')\"}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(\"Warnings:\\n---------\")\n",
    "\n",
    "print(\"\\nDataset-level warnings:\")\n",
    "pprint(cmip6_dataset.warnings)\n",
    "\n",
    "print(\"\\nVariable-level warnings:\")\n",
    "pprint(ua.warnings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c71646c",
   "metadata": {},
   "source": [
    "## 7. Integration with ML packages (PyTorch)\n",
    "\n",
    "The integration with Machine Learning packages should be as seamless as possible, allowing transformations, batching, normalisation and other operations to be defined. The API should allow the user to convert a dataset object directly into a `torch.Dataset` or `tensorflow.Dataset`, ready for use in model training, evaluation or inference.\n",
    "\n",
    "For example, convert to `PyTorch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9d56419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch Dataset created!\n",
      "Dataset length: 360\n",
      "Sample shape: (1, 12, 40, 70)\n",
      "Target shape: (12, 40, 70)\n",
      "Data type: float32\n",
      "\n",
      "üì¶ DataLoader created with batch size 4\n",
      "Number of batches: 90\n",
      "\n",
      "Batch shapes:\n",
      "Features: (4, 1, 12, 40, 70)\n",
      "Targets: (4, 12, 40, 70)\n",
      "Features range: [-4.312, 4.120]\n",
      "Targets range: [-4.197, 4.806]\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch Dataset using GeoCroissant's ML integration\n",
    "climate_dataset = filtered_dataset.to_pytorch_dataset(\n",
    "    target_variable='tas',  # Temperature as target\n",
    "    feature_variables=['tas'],  # Using same variable for demo (can add more)\n",
    "    sequence_length=12,  # 12-month sequences\n",
    "    stride=1,  # Monthly stride\n",
    "    normalize=True,  # Apply standardization\n",
    "    transform='spatiotemporal'  # Prepare for spatiotemporal ML\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ PyTorch Dataset created!\")\n",
    "print(f\"Dataset length: {len(climate_dataset)}\")\n",
    "print(f\"Sample shape: {climate_dataset[0][0].shape}\")  # [features, time, lat, lon]\n",
    "print(f\"Target shape: {climate_dataset[0][1].shape}\")  # [time, lat, lon]\n",
    "print(f\"Data type: {climate_dataset[0][0].dtype}\")\n",
    "\n",
    "# Create DataLoader for training\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(\n",
    "    climate_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"\\nüì¶ DataLoader created with batch size {batch_size}\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")\n",
    "\n",
    "# Inspect a batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "features, targets = sample_batch\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"Features: {features.shape}\")  # [batch, features, time, lat, lon]\n",
    "print(f\"Targets: {targets.shape}\")    # [batch, time, lat, lon]\n",
    "print(f\"Features range: [{features.min():.3f}, {features.max():.3f}]\")\n",
    "print(f\"Targets range: [{targets.min():.3f}, {targets.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8002ad1",
   "metadata": {},
   "source": [
    "Once the data is converted, it can be directly included in a model training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e137652f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model initialized on device: cpu\n",
      "Model architecture: ClimateCNNModel\n",
      "Loss function: MSE Loss\n",
      "Optimizer: Adam (lr=0.001)\n",
      "\n",
      " Starting training simulation...\n",
      "Epoch 1/10 - Average Loss: 0.5000\n",
      "Epoch 2/10 - Average Loss: 0.5000\n",
      "Epoch 3/10 - Average Loss: 0.5000\n",
      "Epoch 4/10 - Average Loss: 0.5000\n",
      "Epoch 5/10 - Average Loss: 0.5000\n",
      "Epoch 6/10 - Average Loss: 0.5000\n",
      "Epoch 7/10 - Average Loss: 0.5000\n",
      "Epoch 8/10 - Average Loss: 0.5000\n",
      "Epoch 9/10 - Average Loss: 0.5000\n",
      "Epoch 10/10 - Average Loss: 0.5000\n",
      "\n",
      "‚úÖ Training simulation completed!\n",
      "Final training loss: 0.5000\n",
      "Model ready for climate prediction tasks\n"
     ]
    }
   ],
   "source": [
    "# Define a simple CNN model for climate prediction\n",
    "class ClimateCNNModel(nn.Module):\n",
    "    def __init__(self, input_channels=1, hidden_dim=64):\n",
    "        super(ClimateCNNModel, self).__init__()\n",
    "        # Mock layers - would normally be actual PyTorch layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(64, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Mock forward pass - just return input reshaped\n",
    "        batch_size = x.shape[0]\n",
    "        return torch.Tensor(np.random.randn(batch_size, 12, 40, 70))\n",
    "    \n",
    "    def to(self, device):\n",
    "        return self  # Mock .to() method\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ClimateCNNModel(input_channels=1).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"‚úÖ Model initialized on device: {device}\")\n",
    "print(f\"Model architecture: ClimateCNNModel\")\n",
    "print(f\"Loss function: MSE Loss\")\n",
    "print(f\"Optimizer: Adam (lr=0.001)\")\n",
    "\n",
    "# Simulate training loop\n",
    "print(f\"\\n Starting training simulation...\")\n",
    "n_epochs = 10\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    # Simulate training over a few batches\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        if batch_idx >= 3:  # Just simulate 3 batches per epoch\n",
    "            break\n",
    "            \n",
    "        # Mock training step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (mock)\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, targets)\n",
    "        \n",
    "        # Mock backward pass\n",
    "        # loss.backward()  # Would normally do backprop\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.min()  # Use min as mock loss value\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / n_batches if n_batches > 0 else 0\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training simulation completed!\")\n",
    "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Model ready for climate prediction tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534eddfb",
   "metadata": {},
   "source": [
    "## 8. Agentic access (via MCP)\n",
    "\n",
    "Model Context Protocol, or MCP (https://modelcontextprotocol.io/docs/getting-started/intro), is an emerging open standard for agentic AI systems to communicate with each other, and with a range of tools.\n",
    "\n",
    "When thinking about data discovery and access, we might choose to expose GeoCroissant functionality within MCP servers, to provide:\n",
    "- Search capability\n",
    "- Extract, Transform and Load capabilities\n",
    "\n",
    "**NOTE: This part of the mock-up has not been fully considered yet. More work to come here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2be2e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mockup for MCP integration coming soon!\n",
    "# It should include:\n",
    "#   - Exposing GeoCroissant capabilities via MCP profiles\n",
    "#   - Enabling agentic search and data retrieval workflows\n",
    "#   - Demonstrating example agent workflows, using LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a67f348",
   "metadata": {},
   "source": [
    "## 9. Accessing local and/or remote data (file system vs S3/HTTP)\n",
    "\n",
    "The model for the GeoCroissant interface is that it should _work the same_ (although the performance will vary) for data that is:\n",
    "- stored **at different sites/services**:\n",
    "  - if data files are on the local file system - it should use the fastest route to the data\n",
    "  - if data files are remote, then it should download them using the appropriate protocol:\n",
    "    - `http(s)`\n",
    "    - `s3`\n",
    "    - other...\n",
    "- stored **in different formats**:\n",
    "  - supported formats will include:\n",
    "    - `NetCDF`\n",
    "    - `GRIB`\n",
    "    - `Zarr`\n",
    "    - `Kerchunk` / `VirtualiZarr` (as aggregation layers over other formats)\n",
    "\n",
    "The most important aspect is that the **recipient format** should match what is needed by the user:\n",
    "- `xarray.DataArray`, `xarray.Dataset` or `xarray.DataTree`.\n",
    "- `numpy.ndarray` objects (easily converted to `torch.Tensor` objects)\n",
    "- others...?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e160b6",
   "metadata": {},
   "source": [
    "## 10. Handling restricted data with access control\n",
    "\n",
    "Since some data may be restricted in access, the API needs to handle access/API tokens and potentially other authorisation tokens that might be passed to the underlying service through:\n",
    "- environment variables\n",
    "- HTTP(S) headers\n",
    "- parameters in Python calls\n",
    "\n",
    "At the simplest level, this should like something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9dcecf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded with authorization token successfully!\n"
     ]
    }
   ],
   "source": [
    "# Get a token from an authentication service, in this case CEDA's token service\n",
    "token = ceda_auth.get_access_token(refresh=True)\n",
    "\n",
    "# Set up the request headers\n",
    "ds = cmip6_dataset.filter(**selection_criteria, auth_token=token)\n",
    "\n",
    "print(\"Data loaded with authorization token successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ea8d7",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 11. Benchmarking\n",
    "\n",
    "As part of the GeoCroissant API, we need to be able to measure the performance of different parts of the system, to ensure the interfaces and transfer mechanisms are optimised.\n",
    "\n",
    "**More to come here about benchmarking.**\n",
    "\n",
    "TO-DO:\n",
    "- Define benchmarks and reproducible tests for performance:\n",
    "  - Common read/load/transform benchmarks (throughput, latency, memory)\n",
    "  - Dataset and hardware profiling guidance\n",
    "  - Reproducible scripts and CI-friendly performance checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f265c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
