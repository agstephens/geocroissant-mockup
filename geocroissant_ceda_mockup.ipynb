{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b073387f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd9ce5ad",
   "metadata": {},
   "source": [
    "# Mockup of use cases and vision for CEDA adoption of GeoCroissant\n",
    "\n",
    "The Centre for Environmental Data Analysis (CEDA), and its partner UK environmental data centres, are working on multiple projects aimed at making their data more _AI-ready_. What we mean by _AI-readiness_ is that the data should be:\n",
    "- easy to find\n",
    "- easy to access\n",
    "- efficient to process/load at scale\n",
    "- integrated with local/remote performant caching\n",
    "- easy to transform and load into Machine Learning workflows\n",
    "- easy for Agentic AI to interact with\n",
    "- self-describing in terms of its characteristics in relation to usage, such as:\n",
    "  - caveats on usage\n",
    "  - consideration of data quality and uncertainty\n",
    "  - clarification of biases in the collection and construction of the data\n",
    "\n",
    "These characteristics are highlighted in the following sections of this Notebook:\n",
    "1. Discover, search and query\n",
    "2. Interrogate the contents of a dataset\n",
    "3. Filter and subset\n",
    "4. Extract, transform and load\n",
    "5. Copying data to a local cache\n",
    "6. Usage warnings and caveats (at _global_ and _variable_ levels)\n",
    "7. Agentic access (via MCP)\n",
    "8. Accessing local and/or remote data (file system vs S3/HTTP)\n",
    "9. Handling restricted data with access control\n",
    "10. Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c33e1",
   "metadata": {},
   "source": [
    "### Firstly, we'll make some imports to set up the Notebook\n",
    "\n",
    "**NOTE: this is a synthetic notebook that uses _mock_ packages. It is intended as a useful tool for describing (and proposing) a narrative on how `geocroissant` might work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82dd9a47",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "\n",
      "Croissant version: 1.2.3\n",
      "PyTorch version: 2.1.0\n",
      "Xarray version: 2023.10.1\n",
      "Cartopy version: 0.22.0\n",
      "STAC Client version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries from the external mock module\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current directory to Python path to find mymock.py\n",
    "current_dir = os.path.dirname(os.path.abspath('.'))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "# Import from our mock module\n",
    "from mymock import (\n",
    "    croissant, torch, xr, STACIntegration, DataLoader, Dataset,\n",
    "    torch_nn as nn, matplotlib_pyplot as plt, cartopy_crs as ccrs, \n",
    "    cartopy_feature as cfeature, pystac_client as Client\n",
    ")\n",
    "\n",
    "# Standard libraries (these are real)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\\n\")\n",
    "print(f\"Croissant version: {croissant.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Xarray version: {xr.__version__}\")\n",
    "print(f\"Cartopy version: 0.22.0\")\n",
    "print(f\"STAC Client version: 0.7.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18604a7",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 1. Discover, search and query\n",
    "\n",
    "At the top level, users should have a single Python API from which they can explore _all data_. In this example, we imagine that there is a `GeoCroissant` object imported from `croissant` that you can create an instance of by giving it the URL to a (Geo-)Croissant catalogue.\n",
    "\n",
    "The end-point serves up _geo-aware_ dataset records that can be interrogated.\n",
    "\n",
    "Note that the `GeoCroissant` object can be interrogated in multiple ways:\n",
    "1. Using a built-in operations for space and time:\n",
    "  - `spatial_coverage`\n",
    "  - `temporal_coverage`\n",
    "2. By keywords - based on those tagged in the datasets\n",
    "3. By _facets_:\n",
    "  - Picking up domain-specific vocabularies for different datasets, such as:\n",
    "    - Satellite data: `sensor_id`, `platform`\n",
    "    - Climate simulations: `ensemble_member`, `grid_type`, `frequency`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af83b986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing GeoCroissant client using provider: https://catalogue.ceda.ac.uk/croissant/\n",
      "Found 3 matching datasets:\n",
      "\n",
      "1. CMIP6_Global_Climate_Projections\n",
      "     Description: Multi-model ensemble of global climate projections from CMIP6\n",
      "     Provider: ESGF Data Nodes\n",
      "     Variables: temperature, precipitation, pressure...\n",
      "     Spatial Resolution: 1.25¬∞ x 1.25¬∞\n",
      "     Temporal Resolution: monthly\n",
      "\n",
      "2. ERA5_Reanalysis_Global\n",
      "     Description: ECMWF ERA5 atmospheric reanalysis dataset\n",
      "     Provider: Copernicus Climate Data Store\n",
      "     Variables: temperature, wind, pressure...\n",
      "     Spatial Resolution: 0.25¬∞ x 0.25¬∞\n",
      "     Temporal Resolution: hourly\n",
      "\n",
      "3. MODIS_Land_Surface_Temperature\n",
      "     Description: MODIS satellite-derived land surface temperature\n",
      "     Provider: NASA EARTHDATA\n",
      "     Variables: land_surface_temperature, emissivity...\n",
      "     Spatial Resolution: 1km\n",
      "     Temporal Resolution: daily\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize GeoCroissant with multiple data sources\n",
    "geocat = croissant.GeoCroissant(provider=\"https://catalogue.ceda.ac.uk/croissant/\")\n",
    "\n",
    "# Search for climate datasets\n",
    "datasets = geocat.search(\n",
    "    spatial_coverage=[-30, -10, 40, 30],  # Example bounding box [min_lon, min_lat, max_lon, max_lat]\n",
    "    temporal_range=(\"2015-01-01\", \"2100-12-31\"),\n",
    "    keywords=[\"climate\", \"temperature\", \"precipitation\"],\n",
    "    # facets={\"model\": [\"UKESM1-0-LL\", \"HadGEM3-GC31-LL\"]},\n",
    ")\n",
    "\n",
    "print(f\"Found {len(datasets)} matching datasets:\\n\")\n",
    "for i, dataset in enumerate(datasets[:5]):  # Show first 5\n",
    "    print(f\"{i+1}. {dataset.name}\")\n",
    "    print(f\"     Description: {dataset.description}\")\n",
    "    print(f\"     Provider: {dataset.provider}\")\n",
    "    print(f\"     Variables: {', '.join(dataset.variables[:3])}...\")\n",
    "    print(f\"     Spatial Resolution: {dataset.spatial_resolution}\")\n",
    "    print(f\"     Temporal Resolution: {dataset.temporal_resolution}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da27603",
   "metadata": {},
   "source": [
    "## 2. Interrogate the contents of a dataset\n",
    "\n",
    "The catalogue and dataset objects expose methods that allow the user to directly interrogate them regarding their contents. \n",
    "\n",
    "Initially, `<dataset>.get_props(\"__available__\")` returns a list of the possible properties (or _facets_) that the dataset exposes. After that call, the user can use `<dataset>.get_props(\"<prop_name>\")` to find out which values can be selected for each property.\n",
    "\n",
    "**NOTE: A warning appears to provide guidance on how the data can/cannot be used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ba22f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background:#ffe6e6;border:2px solid #d32f2f;padding:16px;border-radius:8px;text-align:center;\">\n",
       "            <span style=\"color:#d32f2f;font-size:1.5em;font-weight:bold;\">‚ö†Ô∏è Important information about the CMIP6 Dataset</span><br>\n",
       "            <span style=\"color:#333;font-size:1.1em;\">The CMIP6 Dataset has the following important factors:\n",
       "\n",
       "    - It is a multi-model ensemble of global climate projections.\n",
       "    - The dataset includes variables such as temperature, precipitation, and wind.\n",
       "    - It is available at a spatial resolution of 1.25¬∞ x 1.25¬∞.\n",
       "    - Different models will have varying temporal coverages and spatial resolutions.\n",
       "    See: more information at <a href=\"https://esgf-node.llnl.gov/projects/cmip6/\">https://esgf-node.llnl.gov/projects/cmip6/</a>\n",
       "        </span>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Interrogating CMIP6 dataset contents...\n",
      "üîß Other Available Properties:\n",
      "Available props: models, experiments, variables, frequencies, realms, institutions, grids, time_ranges\n",
      "Use cmip6_dataset.get_props('property_name') to explore any of these:\n",
      "üìä Available Climate Models (5):\n",
      "  - CESM2: NCAR\n",
      "    Resolution: 0.9x1.25 deg\n",
      "    Experiments: 3\n",
      "\n",
      "  - GFDL-ESM4: NOAA-GFDL\n",
      "    Resolution: 0.5 deg\n",
      "    Experiments: 4\n",
      "\n",
      "  - UKESM1-0-LL: MOHC\n",
      "    Resolution: 1.25x1.875 deg\n",
      "    Experiments: 3\n",
      "\n",
      "  - IPSL-CM6A-LR: IPSL\n",
      "    Resolution: 1.27x2.5 deg\n",
      "    Experiments: 4\n",
      "\n",
      "  - MPI-ESM1-2-HR: MPI-M\n",
      "    Resolution: 0.94x0.94 deg\n",
      "    Experiments: 3\n",
      "\n",
      "üß™ Available Experiments (5):\n",
      "  - ssp126: Low emissions scenario\n",
      "    Activity: ScenarioMIP\n",
      "    Models: 12\n",
      "\n",
      "  - ssp245: Medium emissions scenario\n",
      "    Activity: ScenarioMIP\n",
      "    Models: 15\n",
      "\n",
      "  - ssp370: Medium-high emissions scenario\n",
      "    Activity: ScenarioMIP\n",
      "    Models: 8\n",
      "\n",
      "  - ssp585: High emissions scenario\n",
      "    Activity: ScenarioMIP\n",
      "    Models: 18\n",
      "\n",
      "  - historical: Historical simulation\n",
      "    Activity: CMIP\n",
      "    Models: 25\n",
      "\n",
      "üå°Ô∏è Available Variables (10):\n",
      "  - tas: Near-Surface Air Temperature\n",
      "    Units: K\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - pr: Precipitation\n",
      "    Units: kg m-2 s-1\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - psl: Sea Level Pressure\n",
      "    Units: Pa\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - ua: Eastward Near-Surface Wind\n",
      "    Units: m s-1\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - va: Northward Near-Surface Wind\n",
      "    Units: m s-1\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - huss: Near-Surface Specific Humidity\n",
      "    Units: 1\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - zg: Geopotential Height\n",
      "    Units: m\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'plev', 'lat', 'lon']\n",
      "\n",
      "  - evspsbl: Evaporation\n",
      "    Units: kg m-2 s-1\n",
      "    Frequency: mon\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - tasmax: Daily Maximum Near-Surface Air Temperature\n",
      "    Units: K\n",
      "    Frequency: day\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "  - tasmin: Daily Minimum Near-Surface Air Temperature\n",
      "    Units: K\n",
      "    Frequency: day\n",
      "    Dimensions: ['time', 'lat', 'lon']\n",
      "\n",
      "üîß Other Available Properties:\n",
      "Available props: models, experiments, variables, frequencies, realms, institutions, grids, time_ranges\n",
      "Use cmip6_dataset.get_props('property_name') to explore any of these:\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset (e.g., CMIP6)\n",
    "cmip6_dataset = geocat.load_dataset(\"CMIP6_Global_Climate_Projections\")\n",
    "\n",
    "# Use the generic interrogation API to explore the dataset\n",
    "print(\"üîç Interrogating CMIP6 dataset contents...\")\n",
    "\n",
    "# List available properties\n",
    "available_props = cmip6_dataset.get_props(\"__available__\")\n",
    "print(f\"üîß Other Available Properties:\")\n",
    "print(f\"Available props: {', '.join(available_props)}\")\n",
    "print(f\"Use cmip6_dataset.get_props('property_name') to explore any of these:\")\n",
    "\n",
    "# Get available climate models using generic props interface\n",
    "models = cmip6_dataset.get_props(\"models\")\n",
    "print(f\"üìä Available Climate Models ({len(models)}):\")\n",
    "for model in models[:8]:\n",
    "    print(f\"  - {model.name}: {model.institution}\")\n",
    "    print(f\"    Resolution: {model.nominal_resolution}\")\n",
    "    print(f\"    Experiments: {len(model.experiments)}\")\n",
    "    print()\n",
    "\n",
    "# Get available experiments\n",
    "experiments = cmip6_dataset.get_props(\"experiments\")\n",
    "print(f\"üß™ Available Experiments ({len(experiments)}):\")\n",
    "for exp in experiments[:5]:\n",
    "    print(f\"  - {exp.experiment_id}: {exp.description}\")\n",
    "    print(f\"    Activity: {exp.activity_id}\")\n",
    "    print(f\"    Models: {len(exp.participating_models)}\")\n",
    "    print()\n",
    "\n",
    "# Get available variables\n",
    "variables = cmip6_dataset.get_props(\"variables\")\n",
    "print(f\"üå°Ô∏è Available Variables ({len(variables)}):\")\n",
    "for var in variables[:10]:\n",
    "    print(f\"  - {var.variable_id}: {var.long_name}\")\n",
    "    print(f\"    Units: {var.units}\")\n",
    "    print(f\"    Frequency: {var.frequency}\")\n",
    "    print(f\"    Dimensions: {var.dimensions}\")\n",
    "    print()\n",
    "\n",
    "# Show other available properties that can be interrogated\n",
    "available_props = cmip6_dataset.get_props(\"__available__\")\n",
    "print(f\"üîß Other Available Properties:\")\n",
    "print(f\"Available props: {', '.join(available_props)}\")\n",
    "print(f\"Use cmip6_dataset.get_props('property_name') to explore any of these:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3cbc8",
   "metadata": {},
   "source": [
    "## 3. Filter and subset\n",
    "\n",
    "Before any data is actually loaded, the contents of the required dataset can be filtered. This all uses _lazy loading_ which means that the software stores a graph of the required operations which will only be executed when the data arrays themselves are needed (e.g. for model training, analysis or visualisation).\n",
    "\n",
    "Again, this allows the specification of _generic_ properties, such as _space_ and _time_, along with _dataset-specific_ facets such as `model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "245ed933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CMIP6 dataset loaded successfully!\n",
      "Dataset ID: cmip6_global_climate\n",
      "Title: CMIP6 Global Climate Projections\n",
      "Description: Comprehensive climate model data from CMIP6 including temperature, precipitation, and atmospheric variables\n",
      "License: CC-BY-4.0\n",
      "Extent: {'bbox': [-20, 10, 30, 50]}\n",
      "Time Range: {'interval': [['2020-01-01', '2050-12-31']]}\n",
      "\n",
      "üìÅ STAC Catalog Structure:\n",
      "Collections: 3\n",
      "  - temperature: Surface Temperature\n",
      "    Items: 120\n",
      "    Variables: tas, tasmax, tasmin, pr, huss\n",
      "\n",
      "  - precipitation: Precipitation\n",
      "    Items: 120\n",
      "    Variables: pr, prc, prsn, prw, evspsbl\n",
      "\n",
      "  - atmospheric: Atmospheric Variables\n",
      "    Items: 120\n",
      "    Variables: psl, ua, va, zg, hus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CMIP6 dataset with filter options\n",
    "cmip6_dataset = geocat.load_dataset(\n",
    "    \"CMIP6_Global_Climate_Projections\",\n",
    "    spatial_subset=[-20, 10, 30, 50],  # [min_lon, min_lat, max_lon, max_lat]\n",
    "    temporal_subset=(\"2020-01-01\", \"2050-12-31\"),\n",
    "    variables=[\"tas\", \"pr\", \"psl\"],  # Surface air temperature, precipitation and pressure\n",
    "    facets={\"model\": [\"UKESM1-0-LL\", \"HadGEM3-GC31-LL\"]},\n",
    "    suppress_warnings=True,\n",
    ")\n",
    "\n",
    "# The dataset is loaded with STAC integration\n",
    "print(\"‚úÖ CMIP6 dataset loaded successfully!\")\n",
    "print(f\"Dataset ID: {cmip6_dataset.id}\")\n",
    "print(f\"Title: {cmip6_dataset.title}\")\n",
    "print(f\"Description: {cmip6_dataset.description}\")\n",
    "print(f\"License: {cmip6_dataset.license}\")\n",
    "print(f\"Extent: {cmip6_dataset.spatial_extent}\")\n",
    "print(f\"Time Range: {cmip6_dataset.temporal_extent}\")\n",
    "\n",
    "# Show STAC catalog structure\n",
    "print(f\"\\nüìÅ STAC Catalog Structure:\")\n",
    "print(f\"Collections: {len(cmip6_dataset.collections)}\")\n",
    "for collection in cmip6_dataset.collections[:3]:\n",
    "    print(f\"  - {collection.id}: {collection.title}\")\n",
    "    print(f\"    Items: {len(collection.items)}\")\n",
    "    print(f\"    Variables: {', '.join(collection.summaries.get('variables', [])[:5])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729dd429",
   "metadata": {},
   "source": [
    "Or, alternatively, **apply filters after loading a dataset**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b8897ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Applying selection criteria:\n",
      "  model: CESM2\n",
      "  experiment: ssp585\n",
      "  variable: tas\n",
      "  frequency: monthly\n",
      "  spatial_bounds: {'lat': [30, 70], 'lon': [-130, -60]}\n",
      "  temporal_bounds: {'start': '2020-01-01', 'end': '2050-12-31'}\n",
      "\n",
      "üîÑ Filtering dataset...\n",
      "‚úÖ Filtered dataset created!\n",
      "Original size: 1250.0 GB\n",
      "Filtered size: 85.2 GB\n",
      "Reduction: 93.2%\n",
      "\n",
      "üìã Filtered Dataset Structure:\n",
      "Variables: ['tas']\n",
      "Spatial shape: (40, 70)\n",
      "Temporal shape: (372,)\n",
      "Total timesteps: 372\n",
      "Data format: xarray\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset\n",
    "cmip6_dataset = geocat.load_dataset(\"CMIP6_Global_Climate_Projections\", suppress_warnings=True)\n",
    "\n",
    "# Define filtering selection criteria\n",
    "selection_criteria = {\n",
    "    'model': 'CESM2',  # Community Earth System Model\n",
    "    'experiment': 'ssp585',  # High emissions scenario\n",
    "    'variable': 'tas',  # Near-surface air temperature\n",
    "    'frequency': 'monthly',\n",
    "    'spatial_bounds': {\n",
    "        'lat': [30, 70],  # Northern hemisphere focus\n",
    "        'lon': [-130, -60]  # North America\n",
    "    },\n",
    "    'temporal_bounds': {\n",
    "        'start': '2020-01-01',\n",
    "        'end': '2050-12-31'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üéØ Applying selection criteria:\")\n",
    "for key, value in selection_criteria.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Apply the filters using GeoCroissant's filtering API\n",
    "print(\"\\nüîÑ Filtering dataset...\")\n",
    "filtered_dataset = cmip6_dataset.filter(**selection_criteria)\n",
    "\n",
    "# Display summary of the filtered dataset\n",
    "print(f\"‚úÖ Filtered dataset created!\")\n",
    "print(f\"Original size: {cmip6_dataset.estimated_size_gb:.1f} GB\")\n",
    "print(f\"Filtered size: {filtered_dataset.estimated_size_gb:.1f} GB\")\n",
    "print(f\"Reduction: {(1 - filtered_dataset.estimated_size_gb/cmip6_dataset.estimated_size_gb)*100:.1f}%\")\n",
    "\n",
    "# Show the structure of the filtered dataset\n",
    "print(f\"\\nüìã Filtered Dataset Structure:\")\n",
    "print(f\"Variables: {filtered_dataset.variables}\")\n",
    "print(f\"Spatial shape: {filtered_dataset.spatial_shape}\")\n",
    "print(f\"Temporal shape: {filtered_dataset.temporal_shape}\")\n",
    "print(f\"Total timesteps: {filtered_dataset.n_timesteps}\")\n",
    "print(f\"Data format: {filtered_dataset.data_format}\")  # xarray or tensor ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118e425",
   "metadata": {},
   "source": [
    "## 4. Extract, transform and load\n",
    "\n",
    "For use in Machine Learning workflows, the data will often need to be transformed in structure. \n",
    "\n",
    "Transformers can be applied to the `load_dataset(...)` operation, or applied afterwards. In this example, the data is regridded to a 1 degree grid and converted from 64-bit floats (_double_) to 32-bit floats.\n",
    "\n",
    "Additionally, `masked` values are replaced with the mean statistics from each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ebde60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background:#ffe6e6;border:2px solid #d32f2f;padding:16px;border-radius:8px;text-align:center;\">\n",
       "            <span style=\"color:#d32f2f;font-size:1.5em;font-weight:bold;\">‚ö†Ô∏è Important information about the CMIP6 Dataset</span><br>\n",
       "            <span style=\"color:#333;font-size:1.1em;\">The CMIP6 Dataset has the following important factors:\n",
       "\n",
       "    - It is a multi-model ensemble of global climate projections.\n",
       "    - The dataset includes variables such as temperature, precipitation, and wind.\n",
       "    - It is available at a spatial resolution of 1.25¬∞ x 1.25¬∞.\n",
       "    - Different models will have varying temporal coverages and spatial resolutions.\n",
       "    See: more information at <a href=\"https://esgf-node.llnl.gov/projects/cmip6/\">https://esgf-node.llnl.gov/projects/cmip6/</a>\n",
       "        </span>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CMIP6 dataset prepared to load with transformations applied!\n",
      "Transformations: \n",
      "Transformer type: RegridTransformer, Specification: {'target_grid': '1deg'}\n",
      "Transformer type: TypeCoercionTransformer, Specification: {'dtype': 'float32'}\n",
      "Transformer type: MissingValueImputer, Specification: {'strategy': 'mean'}\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset and apply transformations during the conversion\n",
    "from mymock import croissant\n",
    "\n",
    "\n",
    "cmip6_dataset = geocat.load_dataset(\n",
    "    \"CMIP6_Global_Climate_Projections\",\n",
    "    spatial_subset=[-20, 10, 30, 50],  # [min_lon, min_lat, max_lon, max_lat]\n",
    "    temporal_subset=(\"2020-01-01\", \"2050-12-31\"),\n",
    "    variables=[\"tas\", \"pr\", \"psl\"],  # Surface air temperature, precipitation and pressure\n",
    "    facets={\"model\": [\"UKESM1-0-LL\", \"HadGEM3-GC31-LL\"]},\n",
    "    transformers=[\n",
    "        croissant.transformers.RegridTransformer(target_grid=\"1deg\"),  # Regrid to 1 degree\n",
    "        croissant.transformers.TypeCoercionTransformer(dtype=\"float32\"),  # Convert to 32-bit floats\n",
    "        croissant.transformers.MissingValueImputer(strategy=\"mean\")  # Impute missing values with mean\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ CMIP6 dataset prepared to load with transformations applied!\")\n",
    "print(\"Transformations: \")\n",
    "for transformer in cmip6_dataset.transformers:\n",
    "    print(transformer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f48c18",
   "metadata": {},
   "source": [
    "## 5. Copying data to a local cache\n",
    "\n",
    "Since large geospatial datasets may be used for many epochs/iterations of model training, it is sometimes necesary to cache the data on local disk. This can be done by providing a `cache_directory \n",
    "\n",
    "Explain caching strategies to optimize repeated access:\n",
    "- Local on-disk and in-memory caches\n",
    "- Remote cache/backing store (S3, HTTP cache-control)\n",
    "- Versioned cache keys and eviction policies\n",
    "- Integration with tooling like fsspec and zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a23698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a cache directory for storing downloaded data\n",
    "geocat.set_cache_directory(\"/disks/storage/data_cache/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25036ec",
   "metadata": {},
   "source": [
    "## 6. Usage warnings and caveats (at _global_ and _variable_ levels)\n",
    "\n",
    "Show how metadata and warnings are surfaced to users:\n",
    "- Global dataset-level warnings (licence, known biases)\n",
    "- Variable-level caveats (known gaps, quality flags, uncertainty)\n",
    "- Programmatic APIs and human-readable displays for caveats and provenance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc9eabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "534eddfb",
   "metadata": {},
   "source": [
    "## 7. Agentic access (via MCP)\n",
    "\n",
    "Outline patterns for agent/assistant-driven workflows:\n",
    "- Machine-communicable profiles (MCP) describing capabilities and constraints\n",
    "- Safe, auditable endpoints for agent queries and transformations\n",
    "- Example agent workflows and allowed operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be2e8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a67f348",
   "metadata": {},
   "source": [
    "## 8. Accessing local and/or remote data (file system vs S3/HTTP)\n",
    "\n",
    "Provide guidance for unified access:\n",
    "- fsspec-backed paths for local, S3, HTTP, and authenticated stores\n",
    "- Handling credentials and environment-sensitive configs\n",
    "- Performance considerations for remote vs local access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724043c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19e160b6",
   "metadata": {},
   "source": [
    "## 9. Handling restricted data with access control\n",
    "\n",
    "Describe access control patterns and provenance:\n",
    "- Authentication and authorization flows (OAuth, tokens)\n",
    "- Row/column-level and dataset-level access policies\n",
    "- Auditing, logging and secure compute patterns for sensitive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcecf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee2ea8d7",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 10. Benchmarking\n",
    "\n",
    "Define benchmarks and reproducible tests for performance:\n",
    "- Common read/load/transform benchmarks (throughput, latency, memory)\n",
    "- Dataset and hardware profiling guidance\n",
    "- Reproducible scripts and CI-friendly performance checks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
